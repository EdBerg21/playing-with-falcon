{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmdT9rf1pSNVxcumdNjV1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vilsonrodrigues/playing-with-falcon/blob/main/notebooks/falcon_tgi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References: \n",
        "\n",
        "https://github.com/huggingface/text-generation-inference"
      ],
      "metadata": {
        "id": "2X8SUHvMvTVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On-premise"
      ],
      "metadata": {
        "id": "0QNAfi-140YI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0j0elnkvMiA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "docker run --gpus all --shm-size 1g -p 8080:80 -v $PWD/data:/data \n",
        "     ghcr.io/huggingface/text-generation-inference:0.8 \\ \n",
        "     --model-id tiiuae/falcon-7b-instruct \\ \n",
        "     --num-shard 1  \\ \n",
        "     --quantize bitsandbytes     \n",
        "\"\"\"     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bash"
      ],
      "metadata": {
        "id": "EzdxAQ2zJ--k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "curl 127.0.0.1:8080/generate \\\n",
        "     -X POST \\\n",
        "     -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":17}}' \\\n",
        "     -H 'Content-Type: application/json'\n",
        "\n",
        "curl 127.0.0.1:8080/generate_stream \\\n",
        "    -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\"parameters\":{\"max_new_tokens\":17}}' \\\n",
        "    -H 'Content-Type: application/json'\n",
        "\n",
        "curl 127.0.0.1:8080/ \\\n",
        "    -X POST \\\n",
        "    -d '{\"inputs\":\"What is Deep Learning?\",\n",
        "          \"parameters\":{\"max_new_tokens\":17},\n",
        "          \"stream\": True}' \\\n",
        "    -H 'Content-Type: application/json'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-SfCo-sBJ-hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGI Client"
      ],
      "metadata": {
        "id": "dhN-cYFYKWWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install text-generation"
      ],
      "metadata": {
        "id": "HFaVKp8YKU34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from text_generation import Client\n",
        "\n",
        "# Generate\n",
        "client = Client(\"http://127.0.0.1:8080\")\n",
        "print(client.generate(\"What is Deep Learning?\", max_new_tokens=17).generated_text)\n",
        "\n",
        "# Generate stream\n",
        "text = \"\"\n",
        "for response in client.generate_stream(\"What is Deep Learning?\", max_new_tokens=17):\n",
        "    if not response.token.special:\n",
        "        text += response.token.text\n",
        "print(text)"
      ],
      "metadata": {
        "id": "kh9H_kRyKf9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain"
      ],
      "metadata": {
        "id": "oHGXzzXCKi1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain transformers"
      ],
      "metadata": {
        "id": "gmrkZksFKkbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrapper to TGI client with langchain\n",
        "\n",
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "\n",
        "inference_server_url_local = \"http://127.0.0.1:8080\"\n",
        "\n",
        "llm_local = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_server_url_local,\n",
        "    max_new_tokens=1000,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "id": "KM7Vi4P8K6mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template, \n",
        "    input_variables= [\"question\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm_local)"
      ],
      "metadata": {
        "id": "pHWNDoZaMXSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain(\"your question\")"
      ],
      "metadata": {
        "id": "WxTX3BohMjAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Pod"
      ],
      "metadata": {
        "id": "awxPhMTFMsXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install runpod"
      ],
      "metadata": {
        "id": "G4WBYp63Mw0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import runpod\n",
        "\n",
        "# your key\n",
        "runpod.api_key '...'"
      ],
      "metadata": {
        "id": "8s8n1SiCVw5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_shard = 1\n",
        "model_id = \"tiiuae/falcon-7b-instruct\"\n",
        "quantize = \"bitsandbytes\"\n",
        "\n",
        "pod = runpod.create_pod(\n",
        "    name=\"Falcon-7B-Instruct-POD\",\n",
        "    image_name=\"ghcr.io/huggingface/text-generation-inference:0.8\",\n",
        "    gpu_type_id=\"NVIDIA GeForce RTX 4080\",\n",
        "    cloud_type=\"COMMUNITY\",\n",
        "    docker_args=f\"--model-id {model_id} --num-shard {gpu_count} --quantize {quantize}\",\n",
        "    gpu_count=num_shard,\n",
        "    volume_in_gb=50,\n",
        "    container_disk_in_gb=5,\n",
        "    ports=\"80/http\",\n",
        "    volume_mount_path=\"/data\",\n",
        ")"
      ],
      "metadata": {
        "id": "Vz0BQ8BLW7S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "\n",
        "inference_server_url_cloud = f\"https://{pod[\"id\"]}-80.proxy.runpod.net\"\n",
        "\n",
        "llm_cloud = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_server_url_cloud,\n",
        "    max_new_tokens=1000,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.3,\n",
        "    repetition_penalty=1.03,\n",
        ")"
      ],
      "metadata": {
        "id": "BILLiwN_ZQBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain_cloud = LLMChain(prompt=prompt, llm=llm_cloud)"
      ],
      "metadata": {
        "id": "f821V3hrZjBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain_cloud(\"your new question to falcon\")"
      ],
      "metadata": {
        "id": "Uim-6E2Gbh3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop pod\n",
        "runpod.stop_pod(pod[\"id\"])"
      ],
      "metadata": {
        "id": "xLk0DvIXcl2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# terminate\n",
        "runpod.terminate_pod(pod[\"id\"])"
      ],
      "metadata": {
        "id": "r6zLY333cotl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AWS Support"
      ],
      "metadata": {
        "id": "bYxXY-Nz8mFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aws.amazon.com/pt/blogs/machine-learning/announcing-the-launch-of-new-hugging-face-llm-inference-containers-on-amazon-sagemaker/"
      ],
      "metadata": {
        "id": "ljVgI3BU49V_"
      }
    }
  ]
}